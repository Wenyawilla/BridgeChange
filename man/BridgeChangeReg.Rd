% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BridgeChangeReg.r
\name{BridgeChangeReg}
\alias{BridgeChangeReg}
\title{Bridge Regression with Change-point}
\usage{
BridgeChangeReg(y, X, n.break = 0, scale.data = TRUE, intercept = TRUE,
  mcmc = 100, burn = 100, verbose = 100, thin = 1, c0 = 0.1,
  d0 = 0.1, nu.shape = 2, nu.rate = 2, beta.start = NULL,
  known.alpha = FALSE, alpha.start = 1, alpha.limit = FALSE,
  alpha.MH = TRUE, beta.alg = "SVD", Waic = FALSE, marginal = FALSE)
}
\arguments{
\item{y}{Outcome vector.}

\item{X}{Design matrix. Columns correspond to variables.}

\item{n.break}{The number of change-point(s).
If \code{n.break = 0}, the model corresponds to the usual regression.
Default is \code{n.break = 0}.}

\item{scale.data}{If \code{TRUE}, \code{y} and \code{X} are both scaled to have zero mean and unit variance.
Default is \code{TRUE}.
We recommend \code{scale.data = TRUE} unless the original data are already scaled.}

\item{intercept}{If \code{TRUE}, estimate intercept by MLE.
Default is \code{TRUE}.
This option does not affect the result when \code{n.break = 0}.
We recommend \code{intercept = TRUE} when the number of break is not zero (change-point estimation).}

\item{mcmc}{The number of iterations for gibbs updates. Default is 100.}

\item{burn}{The number of burn-in periods for gibbs updates. Default is 100.}

\item{verbose}{Iterations at which the results are printed on the console. Default is every 100th iteration.}

\item{thin}{Thinning for gibbs updates. Default is 1 (no thinning).}

\item{c0}{Scale parameter for Gamma distribution. Used for the prior of \eqn{\sigma^2}.
Default is 0.1.}

\item{d0}{Shape parameter for Gamma distribution. Used for the prior of \eqn{\sigma^2}.
Default is 0.1.}

\item{nu.shape}{Shape parameter for Gamma distribution. Used for the prior for \eqn{\tau}.
Default is 2.0.}

\item{nu.rate}{Rate parameter for Gamma distribution. Used for the prior for \eqn{\tau}.
Default is 2.0.}

\item{beta.start}{Starting values of beta. If \code{NULL}, randomly choose beta.start from OLS or standard normal distribution.
Default is \code{NULL}.}

\item{known.alpha}{If \code{TRUE}, a user must specify a numeric value \eqn{[0, 2]} in \code{alpha.start}.
Default is \code{FALSE} and therefore \eqn{\alpha} will be estimated.}

\item{alpha.start}{Starting value for alpha.
When \code{known.alpha = TRUE}, alpha is fixed to the value of this argument.
Default is 1.}

\item{alpha.limit}{If \code{TRUE}, alpha is sampled from \eqn{(0,1)}, otherwise alpha is sampled between \eqn{(0,2)}.
Default is \code{FALSE}.}

\item{alpha.MH}{If \code{TRUE}, alpha is updated by the Metropolisâ€“Hastings algorithm.
If \code{FALSE} the Griddy gibbs sampler is used instead.
Default is \code{TRUE}.}

\item{beta.alg}{An algorithm to sample beta.
Default is \code{beta.alg = "SVD"}.
Also supported is \code{beta.alg = "BCK"} and \code{beta.alg = "CHL"}.}

\item{Waic}{If \code{TRUE}, WAIC is computed after the parameter estimation.
Default is \code{FALSE}.}

\item{marginal}{If \code{TRUE}, the marginal likelihood is computed based on Chib's method.
Default is \code{FALSE}.}
}
\value{
An mcmc object that contains the posterior sample of coefficients and variances as columns.
 Rows correspond to mcmc samples. This object can be summarized by functions provided by the coda package.
 The object contains an attribute \code{"intercept"} that stores mcmc samples for intercept and an attribute state storage matrix that contains posterior samples of hidden states.
}
\description{
Univariate response change-point model with Bridge prior.
}
\examples{
####################################
## no change
####################################
set.seed(1999)
mcmc = burn = 1000; thin=1; verbose=100;

## simulate data 
K <- 200
n <- 100
X <- matrix(rnorm(n*K), n, K)
sig2 <- .2
beta.hat <- matrix(rnorm(K), K, 1)*5
beta.hat[sample(1:K, K/2, replace=FALSE)] <- 0
Y <- X\%*\%beta.hat + rnorm(n, 0, sqrt(sig2))



out0 <- BridgeChangeReg(y = Y, X = X, 
                        scale.data = TRUE
                        mcmc = mcmc, burn = burn, thin = thin, verbose = verbose,
                        alpha.MH = TRUE, n.break = 0)
out11 <- BridgeChangeReg(y=Y, X=X, scale.data=FALSE,
                        mcmc=mcmc, intercept = TRUE,
                        burn = burn, thin=thin, verbose=verbose, known.alpha = 1,
                        alpha.MH=TRUE, n.break = 1)

X.sd <- apply(X, 2, sd)
y.sd <- sd(Y)
## normalized true.beta
beta.true <-  beta.hat*(X.sd/y.sd)

## estimate
beta.svd <- (apply(out0[, 1:200], 2, mean) + apply(X, 2, mean)) * apply(X, 2, sd)

## plot
plot(beta.svd, beta.true, ylab="TRUE", xlab="EST", type = "n",
       xlim = range(beta.svd), ylim = range(beta.svd))
points(beta.svd, beta.true, col="blue")
abline(a=0, b=1, col="red")

# ####################################
# ## one change with no correlation
# ####################################
# set.seed(1973);
# sim <- SparseChangeSim(ntime=100, predictor = 100, rho=0.0, constant.p = 0, sign.change.tune = 3, 
#                        positive.jump=FALSE, varying.p = 0.2, break.point = 0.5, dgp.only=TRUE)
# 
# plot(1:length(sim$true.beta[1,]), sim$true.beta[1,], xlab="predictor", ylab="coefficients", ylim=range(sim$true.beta), type='n')
# points(sim$true.beta[1,], col="red", pch="1", cex=1)
# points(sim$true.beta[2,], col="blue", pch="2", cex=1)
# 
# ## slope change
# set.seed(11173);
# out0 <- SparseChangeReg(y=sim$y.c, X=sim$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=FALSE,
#                         burn = burn, thin=thin, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 0, Waic=TRUE, marginal = TRUE)
# 
# set.seed(11173);
# out1 <- SparseChangeReg(y=sim$y, X=sim$x, scale.data=TRUE,
#                         mcmc=100, demean=FALSE, intercept=FALSE,
#                         burn = 100, thin=1, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 1, Waic=TRUE, marginal = TRUE)
# 
# WaicCompare(list(out0, out1))
# MarginalCompare(list(out0, out1))
# 
# SparseChange:::plotCoef(out1, scale.back = TRUE, true.beta = sim$true.beta)
# 
# 
# ## y=out$y.c; X=out$x.c; scale.data=TRUE;
# ## alpha.MH=TRUE; n.break = 0; Waic=TRUE; marginal = TRUE
# ## n.break = 1; scale.data = TRUE;
# ## mcmc = 100; burn = 100; verbose = 100; thin = 1;
# ## c0 = 0.1; d0 = 0.1; a = NULL; b = NULL; demean=FALSE; intercept=TRUE;
# ## beta.start = NULL; nu.shape=2.0; nu.rate=2.0; known.s = FALSE;
# ## alpha.start = 1; known.alpha = FALSE;
# 
# ## beta.st   <- matrix(apply(out1[, 1:400], 2, mean), ns, K, byrow=TRUE) ## matrix(apply(betadraws, 2, mean), ns, K, byrow=TRUE)
# ## lambda.st <- matrix(apply(attr(out1, "lambda"), 2, mean), ns, K, byrow=TRUE)
# ## sig2.st   <- apply(out1[, 401:402], 2, mean)
# ## alpha.st  <- apply(attr(out1, "alpha"), 2, mean)## apply(alphadraws, 2, mean)
# ## tau.st    <- apply(attr(out1, "tau"), 2, mean)## apply(taudraws, 2, mean)
# ## if(n.break > 0){
# ##     P.st <- apply(Pmat, 2, mean)
# ## }
# ## mu.st.state <- out$x.c \%*\% t(beta.st)
# 
# ####################################
# ## one change with small correlation
# ####################################
# set.seed(1973);
# mcmc = burn = 1000; thin=1; verbose=100;
# out <- SparseChangeSim(ntime=100, predictor = 120, rho=0.2, constant.p =0,
#                        positive.jump=FALSE, varying.p = 0.2, break.point = 0.5, dgp.only=TRUE)
# 
# plot(1:length(out$true.beta[1,]), out$true.beta[1,], xlab="predictor", ylab="coefficients", ylim=range(out$true.beta), type='n')
# points(out$true.beta[1,], col="red", pch="1", cex=1)
# points(out$true.beta[2,], col="blue", pch="2", cex=1)
# 
# set.seed(11173);
# out0 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=FALSE,
#                         burn = burn, thin=thin, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 0, Waic=TRUE, marginal = TRUE)
# 
# set.seed(11173);
# out1 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=FALSE,
#                         burn = burn, thin=thin, verbose=verbose,  
#                         alpha.MH=TRUE, n.break = 1, Waic=TRUE, marginal = TRUE)
# 
# WaicCompare(list(out0, out1))
# MarginalCompare(list(out0, out1))
# 
# SparseChange:::plotCoef(out1, true.beta = out$true.beta)
# 
# ####################################
# ## one change with large correlation
# ####################################
# set.seed(1973);
# mcmc = burn = 1000; thin=1; verbose=100;
# out <- SparseChangeSim(ntime=100, predictor = 120, rho=0.5, constant.p =0,
#                        positive.jump=FALSE, varying.p = 0.2, break.point = 0.5, dgp.only=TRUE)
# 
# plot(1:length(out$true.beta[1,]), out$true.beta[1,], xlab="predictor", ylab="coefficients", ylim=range(out$true.beta), type='n')
# points(out$true.beta[1,], col="red", pch="1", cex=1)
# points(out$true.beta[2,], col="blue", pch="2", cex=1)
# 
# set.seed(11173);
# out0 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=FALSE,
#                         burn = burn, thin=thin, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 0, Waic=TRUE, marginal = TRUE)
# 
# set.seed(11173);
# out1 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=FALSE,
#                         burn = burn, thin=thin, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 1, Waic=TRUE, marginal = TRUE)
# 
# set.seed(11173);
# ## lasso-style constraint
# out11 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=FALSE,
#                         burn = burn, thin=thin, verbose=verbose, known.alpha=1,
#                         alpha.MH=TRUE, n.break = 1, Waic=TRUE, marginal = TRUE)
# 
# WaicCompare(list(out0, out1, out11))
# MarginalCompare(list(out0, out1, out11))
# 
# SparseChange:::plotCoef(out11, true.beta = as.vector(out$true.beta))
# 
# ########################################################################
# ## one change with positive slope increases
# ########################################################################
# set.seed(1973);
# mcmc = burn = 1000; thin=1; verbose=100;
# out <- SparseChangeSim(ntime=100, predictor = 120, rho=0, constant.p =0,
#                        positive.jump=TRUE, varying.p = 0.2, break.point = 0.5, dgp.only=TRUE)
# 
# plot(1:length(out$true.beta[1,]), out$true.beta[1,], xlab="predictor", ylab="coefficients", ylim=range(out$true.beta), type='n')
# points(out$true.beta[1,], col="red", pch="1", cex=1)
# points(out$true.beta[2,], col="blue", pch="2", cex=1)
# 
# set.seed(11173);
# out0 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=TRUE,
#                         burn = burn, thin=thin, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 0, Waic=TRUE, marginal = TRUE)
# 
# set.seed(11173);
# out1 <- SparseChangeReg(y=out$y.c, X=out$x.c, scale.data=TRUE,
#                         mcmc=mcmc, demean=TRUE, intercept=TRUE,
#                         burn = burn, thin=thin, verbose=verbose,
#                         alpha.MH=TRUE, n.break = 1, Waic=TRUE, marginal = TRUE)
# 
# WaicCompare(list(out0, out1))
# MarginalCompare(list(out0, out1))
# 
# ## intercept added
# SparseChange:::plotCoef(out1, true.beta = c(0, 0, as.vector(out$true.beta)))
}
